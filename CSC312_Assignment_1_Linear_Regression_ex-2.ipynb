{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CcD8Iv73EEO"
   },
   "source": [
    "# CSC312 – Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIWKraoR3EEP"
   },
   "source": [
    "# Assignment 1 – Linear Regression\n",
    "\n",
    "This is the first assignment of the course. **Extremely important note:** You HAVE to run this notebook with a **Python 3 kernel**; running it with a Python 2 kernel will yield very unpredictable results, and your potentially correct solutions may noted as being wrong because of this fact. So make sure to run it with a **Python 3 kernel**.\n",
    "\n",
    "Given that this is the first practical, I've prepared this interactive notebook for you. The notebook gives you tasks to do which involve, in most or all cases, filling out the body of a function that does something specific, and after each task that is to be marked (except for the plotting questions and the learning rate questions), I've provided a feedback cell that you can select and run that will provide you with instant feedback about whether your work appears to be doing the right thing just to give you a sense of your progress for your personal benefit. Essentially, the feedback cells call the function that you will have to fill out with code and check to see whether the output is apparently correct.\n",
    "\n",
    "***\n",
    "\n",
    "Please note that the interative feedback cells **do not update automatically**. They only execute (and update) once you run them. So you can either select a specific cell after a problem and run it, or you can run all the cells. Either way, after every change, you will need to run the relevant feedback cell(s) to check your progress.\n",
    "\n",
    "Also note that if you haven't completed a task yet at all, running its interactive feedback cell may throw an error. This is, for example, because (amongst other things) it is calling a function that you should have completed that hasn't been completed yet. So if that happens, now you know why.\n",
    "\n",
    "If things get messy, you can always clear the output cells to clean things up. You can do this by means of the ```Edit``` $\\rightarrow$ ```Clear Outputs of All Cells``` menu entry. Don't worry: this won't delete any of your code, but only the output that has been produced by running that code.\n",
    "\n",
    "You need to work on this sheet from the top going sequentially down. In many (probably all) cases, cells lower down depend on functions/variables created and correctly set higher up in the sheet. This will prevent confusion and will help ensure that the notebook works correctly.\n",
    "\n",
    "Make 100% sure to NOT hard-code values for the data sets you've been given here and also to follow the instructions very carefully. This includes things like the number of samples (```m```) and, for multivariate linear regression, the number of features. In almost all cases, the interactive feedback cells use other (potentially multi-dimensional) data with your function to make sure that it is flexible. Use this assignment as an opportunity to learn and apply everything you’ve learnt in class. When marking the work, I will 100% use different datasets, and hard-coding will result in disappointment and low marks. Ok, now on to the assignment.\n",
    "\n",
    "The assignment consists of two parts with two different data sets. The first part deals with univariate (simple) linear regression while the second part delves into multivariate linear regression. Follow the progression of the sheet to unravel the wonders of linear regression.\n",
    "\n",
    "Note that for the rest of this assignment, I've  clearly indicated the parts that will earn you marks by putting in a **[x marks]** just after the question. In most or all such code cells that you need to fill in, I've provided a template function that you need to complete, and I've also indicated the parts where you need to put in code to make things work; only make changes to the code where I've specified, which in many cases will be between the two comment tags that might say something like **```#Fill in below```** and **```#Stop filling in here```**.  **Don't** make any edits to function templates/declarations/inputs or even to the return statements. My auto-marker will assume the function names, input variables and return variables that are provided below. Only edit the body of the function exactly where specified in each part of the assignment. Please also read the instructions very carefully as some questions may work slightly differently, but the instructions are always clearly given.\n",
    "\n",
    "Also note that my data loading cells assume that the files ```data1.csv``` and ```data2.csv```, as well as ```utils.py``` are in the same directory as this notebook. These files have also been uploaded to iKamva along with this sheet. Download and place in the appropriate folder.\n",
    "\n",
    "#### Submission:\n",
    "\n",
    "You will submit your finished jupyter notebook (```.ipynb```) file to iKamva before the deadline.\n",
    "\n",
    "Please adhere to the following very simple instructions when making a submission:\n",
    "\n",
    "\n",
    "1. Make absolutely sure to fill in your student number in the cell below this one. This is necessary for auto-marking purposes.\n",
    "\n",
    "\n",
    "2. If you've submitted your file multiple times on iKamva, there will be multiple copies of the file on iKamva. You need to make sure that there is only one copy on iKamva which is your final preferred copy. Delete all the others. If you don't delete them, we will have no way of knowing which to mark, and will have to mark one at random.\n",
    "\n",
    "\n",
    "3. **DO NOT** upload the data files (```data1.csv``` and ```data2.csv```) or ```utils.py``` as part of your submission. Only upload your jupyter notebook (```.ipynb```) file.\n",
    "\n",
    "\n",
    "4. **DO NOT** compress or zip your file in anyway. Upload the original raw ```.ipynb``` file.\n",
    "\n",
    "\n",
    "5. If your submission is late, even by a few moments, it will not be accepted or marked. We've talked about this at length. God Lord, I wish students would believe me on this. I literally won't accept it (but we'll both just feel bad). I've given you plenty time to submit this work so please aim to **submit 24 hours before the deadline** to avoid disappointment to do with e.g. lack of Internet connectivity in res, load-shedding, skin-shedding, water-shedding, tsnamis and other natural disasters, taxi strikes, lunar eclipses, solar flares etc. There is absolutely no excuse that will be accepted, given the amount of time I've given you. If your work is late, please refrain from emailing it to me; I will **not** accept your work; period. It's for your own good (you'll thank me one day in the future).\n",
    "\n",
    "\n",
    "Good luck and enjoy this notebook (which took me a **LOOOOOOOOOOOOT** of time to create #TorturedSoul)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl5ZLWJv3EEQ"
   },
   "source": [
    "### VERY IMPORTANT: YOUR STUDENT NUMBER\n",
    "\n",
    "Please set the mystudentnumber variable below to your student number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZe3EsDR3EEQ"
   },
   "outputs": [],
   "source": [
    "MYSTUDENTNUMBER = \"4129581\" #Please set this as a STRING."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtcQT7jv3EER"
   },
   "source": [
    "Some initialization code. Just run it and DON'T remove or change it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCoE1kkN3EER"
   },
   "outputs": [],
   "source": [
    "#Run this snippet before moving on to the next parts\n",
    "\n",
    "DEBUG = 0\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#Initialize environment: DON'T REMOVE!\n",
    "environ_init = \"7721hdjfh56nkf771!190sd1hdjfhjhs0071!skskhb0091hdjfh77262jhdywyyuqkjbsdu0811hdjfhnkf771!hslknhd7612oi7!#daiw@_ebcjfibjd$##*Dkdjhq91hdj1hdjfhfhjhdfi1998nkf771!7nkknkf771!@#!@likkhdh1hdjfhuhs919872863jnjhg23\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzPDatDr3EER"
   },
   "source": [
    "# Part 1 - Univariate (Simple) Linear Regression\n",
    "\n",
    "An ice-cream chain has observed a trend, but needs you to confirm it for them: they believe that as the temperature  (in °C) rises above 20°C on a given day, their ice cream sales profit (in Rand) that day soars. They have collected data in this regard i.e. temperature above 20°C on a given day (in °C) and the associated sweet profit (in 1000s of Rand) that they made that day (and this could be negative meaning they made a sweet loss on that day). They want to obtain a predictive model that can help them plan accordingly and to understand how things work. In this part, you're going to use linear regression to obtain a model that can predict the total profit based on the expected temperature in a day above 20°C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_atOoo8x3EES"
   },
   "source": [
    "## 1.1 Reading in and Readying the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLNiw4U93EES"
   },
   "source": [
    "The first step is to read in the data. The data for this part is provided in a ```csv``` file called ```data1.csv``` which contains two columns of data: the first column represents the total sweet profit in 1000s of thousands of Rand (which is what we're trying to predict); the second column represents the temperature above 20°C (which is the feature we're using to make predictions). I've graciously put in the code to import ```numpy``` and to import the data as a ```numpy``` array into the variable ```DATA``` (because the focus here is on machine learning, not on data import)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqP2pUuJ3EES"
   },
   "outputs": [],
   "source": [
    "#Run this snippet before moving on to the next parts\n",
    "#Don't change anything in this code snippet. But do look through it to see what it's doing.\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True) #This just prevents numpy from jumping into scientific display mode for very large/small numbers\n",
    "\n",
    "DATA = np.loadtxt('data1.csv',delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FClTIIw3EES"
   },
   "source": [
    "The data file consists of two columns. The data is displayed below to demonstrate that it is read in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fg3lYZuP3EES",
    "outputId": "c20a436c-c9ef-4620-e74d-7e07a3c167a5"
   },
   "outputs": [],
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgAF_AZk3EET"
   },
   "source": [
    "## 1.2 Setting ```m``` and ```n```\n",
    "\n",
    "Fill in code in the function template ```obtainNandM(data)``` below that accepts, as input, a data array ```data``` *with an arbitrary number of rows and columns*; the function then returns the number of examples ```m``` and the number of features ```n```, respectively.\n",
    "\n",
    "Assume that the first column of ```data``` will always contain the output variable $y$, while the remaining columns (which can be one or more) represent the feature(s) $X$. Note, then, that ```m``` will be the number of rows in ```data``` while ```n``` will be one less than the number of columns in ```data``` because the data format we're using here has the first column as the output and the remaining columns are the features. This function will be marked.\n",
    "\n",
    "**[0.5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2DXPX8c3EET"
   },
   "outputs": [],
   "source": [
    "def obtainNandM(data):\n",
    "\n",
    "    n = 0 #Set this correctly below\n",
    "    m = 0 #Set this correctly below\n",
    "\n",
    "    #Fill in below\n",
    "    n = data.shape[1]-1\n",
    "    m = data.shape[0]\n",
    "    #Stop filling in here\n",
    "    return n,m\n",
    "\n",
    "N,M = obtainNandM(DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR1n9SCb3EET"
   },
   "source": [
    "Note that if you run ```obtainNandM``` by passing in ```DATA``` that we read in from the csv file before, you should end up with $n=1$ and $m=87$ because it contains 1 feature and 87 examples. But note that if you were to pass in a data variable with more features, it should accomodate that. E.g. if you passed in the data array below, you *should* end up with $n=4$ and $m=14$:\n",
    "\n",
    "```DATANFEAT = np.array([[18, 7,65,51,93], [18,34,23,49,30], [48,69,50,89,13], [35,44,86,98,36], [28,32,74,58,11], [ 4,42,97,97,54], [52,19,75,46, 2], [70, 2,21,76,57], [10,73,43,44,71], [74,33, 5,62,51], [ 2,51, 3,40, 6], [76,34,17, 2,89], [76, 9,45,55,74], [ 3,75,96,60,94]])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PS8WztaP3EET",
    "outputId": "54b4d150-7126-489c-919f-c7ffa4da2ee9"
   },
   "outputs": [],
   "source": [
    "#This cell is for YOU to try out your function\n",
    "DATANFEAT = np.array([[18, 7,65,51,93], [18,34,23,49,30], [48,69,50,89,13], [35,44,86,98,36], [28,32,74,58,11], [ 4,42,97,97,54], [52,19,75,46, 2], [70, 2,21,76,57], [10,73,43,44,71], [74,33, 5,62,51], [ 2,51, 3,40, 6], [76,34,17, 2,89], [76, 9,45,55,74], [ 3,75,96,60,94]])\n",
    "\n",
    "def obtainNandM(data):\n",
    "\n",
    "    n = 0 #Set this correctly below\n",
    "    m = 0 #Set this correctly below\n",
    "\n",
    "    #Fill in below\n",
    "    n = data.shape[1]-1\n",
    "    m = data.shape[0]\n",
    "    print(n,m)\n",
    "    #Stop filling in here\n",
    "    return n,m\n",
    "\n",
    "N,M = obtainNandM(DATA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LawFgvbz3EET"
   },
   "source": [
    "Select and run the cell below to see whether your code working correctly. The cell below indicates whether your ```obtainNandM``` function apparently works correctly at this point. Note that the function is tested on other multi-dimensional data to ensure it is flexible. Anyway, run it and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fa6vfHE03EEU",
    "outputId": "8618e810-ddbb-4044-8da2-224f534b05b0"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params1(obtainNandM)\n",
    "utils.testCell(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v--UWRgR3EEU"
   },
   "source": [
    "## 1.3 Obtaining ```X``` and ```y```\n",
    "\n",
    "Now fill in the function template ```obtainYandX(data)``` below to set the variables ```x``` (containing the feature(s)) and ```y``` (containing the intended output). Note that ```y``` will be the first column in the data, and ```x``` will be all remaining columns (whether it is one or more).\n",
    "\n",
    "**Note:** When indexing into ```data``` in the function below, ensure that you don't necessarily assume that the features $X$ are in the 2nd column; rather assume that features $X$ will be all columns from the second column onwards (however many they may be; either 1 or more).\n",
    "\n",
    "You can do this by indexing into the array ```data```. The following examples on a hypothetical array ```A``` might give you clues on how this can be done:\n",
    "\n",
    "- ```numpy``` indexing for 2D arrays takes the following form: ```A[{row_start}:{row_end},{col_start}:{col_end}]``` where ```row_start``` specifies the starting row index to start from, ```row_end``` specifies the row index to end at (*not* including ```row_end``` itself); the same with ```col_start``` and ```col_end``` for columns. I've put curly brackets around each of the indices because they can be omitted and they are optional (more on this later).\n",
    "\n",
    "    - Therefore, ```A[2:5,0:1]``` means row indices 2 up to and not including row index 5 of column index 0 up to and not including column index 1. So this actually means row indices 2 to 4 of column index 0 only.\n",
    "    - ```A[1:4,1:7]``` means row index 1 up to and not including row index 4 of column indices 1 up to and not including column index 7. So this actually means row indices 1 to 3 of column indices 1 to 6.\n",
    "    - ```A[3:4,1:5]``` means row index 3 only, columns 1 to 4.\n",
    "<br><br>\n",
    "- As mentioned before, ```row_start```, ```row_end```, ```col_start``` and ```col_end``` are all optional and can be omitted. If ```row_start``` or ```col_start``` are omitted, ```numpy``` assumes that we are starting from row/col index 0. If ```row_end``` or ```col_end``` are omitted, ```numpy``` assumes that we are ending at the last row/col index in the array, whatever it may be.\n",
    "    - Therefore, saying ```A[2:,1:3]``` means row indices 2 onwards, and column indices 1 and 2.  \n",
    "    - ```A[3:6,:3]``` means row indices 3 to 5, and column indices 0 to 2.\n",
    "    - ```A[2:,:3]``` means row indices 2 onwards, and column indices from 0 to 2.\n",
    "    - ```A[3:, 5:]``` means rows indices 3 onwards, and column indices 5 onwards\n",
    "    - ```A[:,3:4]``` means all rows and column index 3 only.\n",
    "    - ```A[:5,:]``` means row indices 0 to 4, and all columns.\n",
    "    - ```A[:,:]``` means all rows, and all columns i.e. the whole array. Actually, synonymous with `A`.\n",
    "\n",
    "\n",
    "Keep in mind that when trying to return a single column or row, you should specify the starting column/row index *and* the ending column/row index; if you don't do this, you'll end up with a 1D array which will cause big problems later e.g. if you want to return only column index 2 for all rows, you would say ```A[:,2:3]```.\n",
    "\n",
    "Ok after all that, just a reminder that you need to now fill in the function template ```obtainYandX(data)``` below to set the variables ```x``` (containing the feature(s)) and ```y``` (containing the intended output). Note that ```y``` will be the first column in the data, and ```x``` will be all remaining columns from column index 1 onwards.\n",
    "\n",
    "**[0.5 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0B4ck-T3EEU",
    "outputId": "99e46ae3-8124-4c9e-eca7-8b13a0a2a930"
   },
   "outputs": [],
   "source": [
    "def obtainYandX(data):\n",
    "\n",
    "    n,m = obtainNandM(data) #You might need n and/or m. Use them if you wish\n",
    "\n",
    "    x = 0 #Set this correctly below\n",
    "    y = 0 #Set this correctly below\n",
    "\n",
    "    #Fill in below\n",
    "    y = data[:,0:1]\n",
    "    x = data[:,1:]\n",
    "\n",
    "\n",
    "\n",
    "    #Stop filling in here\n",
    "    return y,x\n",
    "\n",
    "Y,X = obtainYandX(DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIHwlJLd3EEU"
   },
   "source": [
    "Note that if you run ```obtainYandX``` by passing in ```DATA``` that we read in from the csv file before, you should end up with $y$ containing the first column of ```DATA```, and $X$ containing the second column. But note that if you were to pass in a data variable with more features, it should accomodate that. E.g. if you passed in the data array ```DATANFEAT``` below, you *should* end up with $y$ containing the first column and $X$ containing four columns.\n",
    "\n",
    "Given:\n",
    "\n",
    "```DATANFEAT = np.array([[18, 7,65,51,93], [18,34,23,49,30], [48,69,50,89,13], [35,44,86,98,36], [28,32,74,58,11], [ 4,42,97,97,54], [52,19,75,46, 2], [70, 2,21,76,57], [10,73,43,44,71], [74,33, 5,62,51], [ 2,51, 3,40, 6], [76,34,17, 2,89], [76, 9,45,55,74], [ 3,75,96,60,94]])```\n",
    "\n",
    "You should get:\n",
    "\n",
    "$y = $```np.array([[18], [18], [48], [35], [28], [ 4], [52], [70], [10], [74], [ 2], [76], [76], [ 3]])```\n",
    "\n",
    "and\n",
    "\n",
    "$X = $```np.array([[ 7,65,51,93], [34,23,49,30], [69,50,89,13], [44,86,98,36], [32,74,58,11], [42,97,97,54], [19,75,46, 2], [ 2,21,76,57], [73,43,44,71], [33, 5,62,51], [51, 3,40, 6], [34,17, 2,89], [ 9,45,55,74], [75,96,60,94]])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SsSCRRyO3EEV",
    "outputId": "e319deda-4ada-4a3d-b98a-6cc9d2d302cc"
   },
   "outputs": [],
   "source": [
    "#This cell is for YOU to try out your function\n",
    "def obtainYandX(data):\n",
    "\n",
    "    n,m = obtainNandM(data) #You might need n and/or m. Use them if you wish\n",
    "\n",
    "    x = 0 #Set this correctly below\n",
    "    y = 0 #Set this correctly below\n",
    "\n",
    "    #Fill in below\n",
    "    y = data[:,0:1]\n",
    "    x = data[:,1:]\n",
    "    print(y)\n",
    "\n",
    "\n",
    "    #Stop filling in here\n",
    "    return y,x\n",
    "\n",
    "Y,X = obtainYandX(DATA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDg7lqNy3EEV"
   },
   "source": [
    "Select and run the cell below to see whether your code working correctly. The cell below indicates whether your ```obtainYandX``` function apparently works correctly at this point. Note that the function is tested on other multi-dimensional data to ensure it is flexible. Anyway, run it and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0mz5r3Q3EEV",
    "outputId": "923eeb09-c205-4b94-de9c-f4cc7a00faf7"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params2(obtainYandX)\n",
    "utils.testCell(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYWg3yeI3EEV"
   },
   "source": [
    "## 1.4 Plotting the Data\n",
    "\n",
    "Ok now that we have the data (or some of it) available, let's make a plot. Plotting data can come in handy, but as discussed in class, can only really be applied when you've got a single feature. In the function `drawGraph` belowm, use the two matrices ```x``` and ```y``` to plot a scatter plot of Profit versus Temperature rise below. Preferably use``` matplotlib``` to do it. Search engines are your friend (preferably the ones that don't spy on you... wait, do those exist anymore...? Ok, preferably that at least don't blatantly tell you that they spy on you... #fixedit). Note: I've plotted some graphs further below in this worksheet. Maybe you can use those to help you...\n",
    "\n",
    "**[2 marks]**\n",
    "\n",
    "**Important: Don't modify ```x``` or ```y``` in any way in your function.**\n",
    "\n",
    "If you did it correctly, you should see a plot that is somewhat similar to the Figure below. Hopefully you can see that the data is more or less linear. Next, we're going to start to fit a line to the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "zXys-91W3EEV",
    "outputId": "17781000-f144-4713-c982-43ea499624d8"
   },
   "outputs": [],
   "source": [
    "#Fill in below Import any libraries you may need for plotting inside the function here\n",
    "import matplotlib.pyplot as plt\n",
    "#Stop filling in here\n",
    "\n",
    "\n",
    "def drawGraph(y,x):\n",
    "    #Fill in below\n",
    "    fig, ax = plt.subplots(figsize=(5, 3))\n",
    "    ax.scatter(x,y, s=20, facecolor='C0', edgecolor='k')\n",
    "    ax.set_title('Profit vs Temperature (above 20 degrees)')\n",
    "    ax.set_xlabel('Temp (above 20 degrees)')\n",
    "    ax.set_ylabel('Profit (1000s of R)')\n",
    "\n",
    "    #Stop filling in here\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "drawGraph(Y,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFCT6ZGT3EEV"
   },
   "source": [
    "***\n",
    "\n",
    "## 1.5 Iterative Methods for Computing the Cost and Gradient Descent (Slow)\n",
    "\n",
    "We're first going to implement the hypothesis, the cost function and gradient descent using the iterative method (i.e. looping through samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVRorA-z3EEV"
   },
   "source": [
    "### 1.5.1 Computing the Hypothesis With One Feature - Iterative Method\n",
    "\n",
    "The hypothesis is basically a linear sum of a given $\\Theta$ parameters with the features $X$. If we have just one example $x$, it is given by:\n",
    "\n",
    "<center>$\\large h_\\theta(x) = \\theta_1x + \\theta_0$</center>\n",
    "\n",
    "However, if we have more than one example, such as we currently do in our variable ```X``` (where each example is on a single row in ```X``` in this case), we can get the prediction for each example in $X$ as follows:\n",
    "\n",
    "<center>$\\large h_\\theta(x^{(i)}) = \\theta_1x^{(i)} + \\theta_0$</center>\n",
    "\n",
    "When doing this iteratively, we would need to loop through values of $i$ and compute each prediction separately. Go ahead and do exactly this in the function `predHIter` below. I've created an array ```h``` of the same size as ```x``` to store the predictions. You need to loop through values of ```i``` and set each row ```h[i,0]``` of ```h``` correctly.\n",
    "\n",
    "**[0.5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lf-NuNIE3EEV"
   },
   "outputs": [],
   "source": [
    "def predHIter(theta,x,m):\n",
    "\n",
    "    h = np.zeros((m,1)) #Set this variable correctly below - an array h to store the predicted value for each row x\n",
    "\n",
    "    #Fill in below - loop through every row of x (x[i,0]) and compute and set h[i,0].\n",
    "    for i in range(m):\n",
    "      h[i,0]= (theta[0] + np.sum(theta[1:] * x[i,:])).item()\n",
    "\n",
    "    #Stop filling in here\n",
    "\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkFvkcf-3EEW"
   },
   "source": [
    "Note that if you run the function you've written with $\\Theta = (\\theta_0 = 1, \\theta_1 = 2)$ applied to ```X```, and if you did it right, you should be getting (and seeing) a $(87 \\times 1)$ matrix of predicted values, as follows:\n",
    "\n",
    "$\n",
    "[[12.36], [ 8.46], [13.3 ], [18.68], \\ldots , [ 6.12], [ 4.34], [ 6.22], [14.94]]\n",
    "$\n",
    "\n",
    "If not, re-visit the function above. It needs to work correctly for the rest of this assignment to work correctly. Feel free to try out your function for your own purposes in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYiWMM-a3EEW",
    "outputId": "d77b497e-b915-4a83-df88-ce3b542e1eae"
   },
   "outputs": [],
   "source": [
    "#This code snippet is for YOU\n",
    "#It just calls the hypothesis function. Feel free to play around with this to see what your hypothesis function is doing.\n",
    "\n",
    "THETA = np.array([[1],[2]]) #Change this to whatever thetas you like\n",
    "\n",
    "\n",
    "PREDSITER = predHIter(THETA,X,M) #I wouldn't advise changing this\n",
    "print(np.array2string(PREDSITER, separator=',').replace(\"\\n\",\"\")) #I wouldn't advise changing this - I'm using the array2string function to be able to display the array in a more visually accessible format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gQci8wF3EEW"
   },
   "source": [
    "Select and run the cell below to see whether your code working correctly. The cell below indicates whether your ```predHIter``` function apparently works correctly at this point. Note that the function is tested on other data (with only feature) to ensure it is flexible. Anyway, run it and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osXAc8Fw3EEW",
    "outputId": "f317b925-0c41-4adb-fe49-e692edf15c93"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params3(predHIter)\n",
    "utils.testCell(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRfyCU6G3EEW"
   },
   "source": [
    "### 1.5.2 Computing the Cost With One Feature - Iterative Method\n",
    "\n",
    "The cost function is a function that provides an exact measure (i.e. the cost) for how well a given line (the hypothesis) represented in terms of its parameters $\\Theta$ fits a given set of data given by features $X$ and outputs $y$. The cost is given by:\n",
    "\n",
    "<center>$\\large J(\\Theta)=\\frac{1}{2m}\\sum\\limits_{i=0}^{m}\\Big[h_\\theta(x^{(i)}) - y^{(i)})\\Big]^2$</center>\n",
    "\n",
    "where $h_\\theta(x^{(i)}))$ in this case (with only one feature) is given by:\n",
    "\n",
    "<center>$\\large h_\\theta(x^{(i)}) = \\theta_1x^{(i)} + \\theta_0$</center>\n",
    "\n",
    "so the cost function can also be written as:\n",
    "\n",
    "<center>$\\large J(\\Theta)=\\frac{1}{2m}\\sum\\limits_{i=0}^{m}\\Big[\\theta_1x^{(i)} + \\theta_0 - y^{(i)}\\Big]^2$</center>\n",
    "\n",
    "To compute the cost iteratively, one can loop over each of the $m$ samples $(x^{(i)},y^{(i)})$ and:\n",
    "   - Compute $\\theta_1x^{(i)} + \\theta_0 - y^{(i)}$;\n",
    "   - Square the result;\n",
    "   - Accumulate the sum of these values over all $m$ samples;\n",
    "   - And finally divide the final sum by $2m$.\n",
    "\n",
    "\n",
    "Below, go ahead and implement the **iterative solution** I've described above, in the ```getCostIter(theta, x, y, m)``` function I've fleshed out below, and make sure it returns the relevant cost as **a single float value** and NOT a matrix or array of any kind.\n",
    "\n",
    "**[4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUhvVmvx3EEW"
   },
   "outputs": [],
   "source": [
    "def getCostIter(theta, x, y, m):\n",
    "\n",
    "    cost = 0\n",
    "    #Fill in below - there needs to be (at least one) loop in here\n",
    "    addition = 0\n",
    "    for i in range(m):\n",
    "      pred = theta[0,0]\n",
    "      for j in range(x.shape[1]):\n",
    "        pred += theta[j+1,0] * x[i,j]\n",
    "\n",
    "\n",
    "      error = (pred - y[i,0])**2\n",
    "      addition += error\n",
    "    cost = (addition / (2 * m))\n",
    "\n",
    "    #Stop filling in here\n",
    "\n",
    "    return np.squeeze(cost)#the cost as a single value - consider using the float() function just before returning it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "to1PGTzp3EEW"
   },
   "source": [
    "Note that if you run the function you've written with the values $(\\theta_0 = 1.87, \\theta_1 = 1.23)$ (which are the approximate best-fit solutions to the data) and if you did it right, you should be seeing the value (yes a single value and NOT an array) ```0.9289940...```. If not, re-visit the function above. It needs to work correctly for some parts of the rest of this assignment to work correctly. Feel free to try out your function for your own purposes in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCL4tmfe3EEX",
    "outputId": "f60de575-59a1-49fd-9183-4dc165c6bb3e"
   },
   "outputs": [],
   "source": [
    "#This code snippet is for YOU\n",
    "#It just calls the cost function. Feel free to play around with this to see what your cost function is doing.\n",
    "THETA = np.array([[1.87],[1.23]]) #Change this to whatever thetas you like\n",
    "print(getCostIter(THETA, X, Y, M)) #I wouldn't advise changing this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcDU4MDK3EEX"
   },
   "source": [
    "Select and run the cell below to see whether your code is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BTSKuG403EEX",
    "outputId": "04c82c44-793e-4034-8ce0-d035894291f5"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params4(getCostIter)\n",
    "utils.testCell(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqR7l3IY3EEX"
   },
   "source": [
    "### 1.5.3 Implementing Gradient Descent With One Feature - Iterative Method\n",
    "\n",
    "Now we'll move on to implementing gradient descent. Gradient descent is a brilliant generic algorithm that can minimize any function. In this case, we're going to use it to obtain $\\theta_0$ and $\\theta_1$ that fit the data very well. We're going to start with some initial guess of $\\theta_0$ and $\\theta_1$ (e.g. we'll just initially set them to $\\theta_0=0$ and $\\theta_1=0$), and then keep making updates to $\\theta_0$ and $\\theta_1$ such that we're reducing the cost i.e. we're getting $\\Theta$ that fit the data better and better. We *could* repeat this process until the cost is not reducing substantially anymore, but let's rather simply iterate a large number of times (```k_iters```) and hope that that number of times will be enough to ensure convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atiq8rHL3EEX"
   },
   "source": [
    "Below I've fleshed out a function ```gradDescIter(theta, x, y, m, k_iters, lr)```, noting that ```lr``` represents the learning rate $\\alpha$. I've put in the main for loop; you have to fill in code into the for loop to update $\\theta_0$ and $\\theta_1$ simultaneously. Before the function returns, make sure to update the vector ```theta``` with the final $\\theta_0$ and $\\theta_1$ values that your code has computed. You may also update the vector ```theta``` in every iteration. It's up to you. This variable will be returned by the function.\n",
    "\n",
    "Things to keep in mind:\n",
    "- ```numpy``` might mess with your values in `theta` as you update them. Be mindful of this.\n",
    "- ```numpy``` might mess with the types in `theta` as you update them e.g. if `theta` is set to be an integer array, it will simply truncate all decimals and cause ginormous frustration. Be mindful of this.\n",
    "\n",
    "The two updates are given by:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\theta_0 \\leftarrow \\theta_0 - \\frac{\\alpha}{m}\\sum\\limits_{i=0}^{m}(\\theta_1x^{(i)}+\\theta_0 - y^{(i)})\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\large\n",
    "\\theta_1 \\leftarrow \\theta_1 - \\frac{\\alpha}{m}\\sum\\limits_{i=0}^{m}\\big[(\\theta_1x^{(i)}+\\theta_0 - y^{(i)})x^{(i)}\\big]\n",
    "$$\n",
    "\n",
    "I've also put in some code to use the ```getCostIter``` function you created earlier to record the cost at each iteration into the ```J_hist``` list. Don't modify this in any way. Later, we will use this to plot the cost versus iteration to visualize convergence.\n",
    "\n",
    "**[5.5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fwOQHp33EEX"
   },
   "outputs": [],
   "source": [
    "def gradDescIter(theta, x, y, m, k_iters, lr):\n",
    "\n",
    "    J_hist = []\n",
    "\n",
    "    #FILL IN Your code here: make a call to the Iterative Cost Function to get the cost:\n",
    "    costJ = getCostIter(theta, x, y, m) #Set this\n",
    "    #STOP FILLING IN here\n",
    "\n",
    "    J_hist.append(costJ)\n",
    "\n",
    "    for k in range(k_iters):\n",
    "\n",
    "        #FILL IN Your code here: carry out an update to all thetas. There SHOULD be a loop in here\n",
    "        gradient = np.zeros(theta.shape)\n",
    "        for i in range(m):\n",
    "          pred = theta[0,0]\n",
    "          for j in range(x.shape[1]):\n",
    "            pred += theta[j+1,0] * x[i,j]\n",
    "\n",
    "          error = (pred - y[i,0])\n",
    "          gradient[0,0] += error\n",
    "\n",
    "          for j in range(x.shape[1]):\n",
    "            gradient[j+1,0] += error * x[i,j]\n",
    "\n",
    "        for j in range(theta.shape[0]):\n",
    "          theta[j,0] -= (lr/m) * gradient[j,0]\n",
    "\n",
    "        #STOP FILLING IN here\n",
    "\n",
    "        #FILL IN Your code here: make a call to the Iterative Cost Function to get the cost:\n",
    "        costJ = getCostIter(theta, x, y, m)\n",
    "\n",
    "        #STOP FILLING IN here\n",
    "\n",
    "        J_hist.append(costJ)\n",
    "\n",
    "    return J_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9XyII_v3EEY"
   },
   "source": [
    "If you did it correctly, running ```gradDescIter``` with initial $(\\theta_0=0,\\theta_1=0)$ with a ```lr=0.001``` over ```k_iters=100``` iterations, you should expect to end up with values of $\\theta_0 \\approx 0.28669954, \\theta_1 \\approx 1.35706138$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFjNmVlz3EEY"
   },
   "outputs": [],
   "source": [
    "#This code snippet is for YOU\n",
    "#It just calls the gradient descent function. Feel free to play around with this to see what your function is doing.\n",
    "\n",
    "THETA = np.array([[0],[0]]) #Change this to whatever thetas you like\n",
    "LR = 0.001\n",
    "K_ITERS = 100 #Be careful about setting this too large with iterative gradient descent... it can take a LONG time to finish\n",
    "\n",
    "\n",
    "JHISTS, NEWTHETA = gradDescIter(THETA, X, Y, M, K_ITERS, LR) #I wouldn't advise changing this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qovJGth_3EEY"
   },
   "source": [
    "Now as before, select and run the cell below to see whether your code working correctly. The cell calls your function. Since this method is iterative and slow, it only runs it over a small number of iterations (100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhLa9NYx3EEY",
    "outputId": "237c7d0b-6250-43dc-c74e-9f40ada46669"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params5(gradDescIter)\n",
    "utils.testCell(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9b82KYv3EEY"
   },
   "source": [
    "***\n",
    "\n",
    "## 1.6 Vectorized Methods for Computing the Cost and Gradient Descent (Ultra Fast)\n",
    "\n",
    "Now we're going to implement the cost function and gradient descent again, but this time we'll be using the vectorized expressions to compute them (i.e. **NO** looping through samples, and you won't even need to anyway, if you do things as described below). In many cases, this will mean that you will be able to do the same tasks with only a single line of code.\n",
    "\n",
    "### 1.6.1 Adding Feature $\\large x_0$ to $X$\n",
    "\n",
    "Since we're going to be using vectorization, we now need to create a modified $X$ that includes a columns of 1s that represent feature $x_0$. Fill in the function ```addX0(x,m)``` below which takes in an ```x``` along with the number of examples in ```x``` given by ```m``` and returns ```xmod``` which is ```x``` with an extra column of 1s to the left of it. To do this, you can use:\n",
    "\n",
    "- the ```numpy``` function ```ones``` which can create an array consisting of 1s. Make sure this array is (m$\\times$1) dimensional; you can use ```A.shape``` to find out the dimensionality (i.e. shape) of any ```numpy``` array ```A```\n",
    "- the ```numpy``` function ```np.hstack``` which takes in two ```numpy``` arrays and literally stacks them next to one another horizontally i.e. array of 1s next to ```x```.\n",
    "\n",
    "**[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAdrnILE3EEY"
   },
   "outputs": [],
   "source": [
    "def addX0(x,m):\n",
    "\n",
    "    xmod = 0 #You will need to set this correctly below\n",
    "\n",
    "    #Fill in below\n",
    "    ones = np.ones((m,1))\n",
    "    xmod = np.hstack((ones,x))\n",
    "\n",
    "\n",
    "    #Stop filling in here\n",
    "\n",
    "    return xmod\n",
    "\n",
    "XMOD = addX0(X,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QK7XtRCt3EEY"
   },
   "source": [
    "Select and run the cell below to see whether your code is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zu7F6_Nk3EEY",
    "outputId": "86799b68-06fd-440a-fb23-9ec635e7b061"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params6(addX0)\n",
    "utils.testCell(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9N7dz4t3EEY"
   },
   "source": [
    "### 1.6.2 Computing the Hypothesis - Vectorized Method\n",
    "\n",
    "The hypothesis is basically a linear sum of a given $\\Theta$ parameters with the features $X$. Regardless of how many features are in $X$ (and corresponding weights/parameters in $\\Theta$), the following vectorized expression can be used to compute predictions for ALL $X$ samples in one go:\n",
    "\n",
    "<br>\n",
    "<center>$\\large h_\\theta(X) = X\\Theta$</center>\n",
    "\n",
    "Noting that $X$ here is actually ```XMOD``` i.e. it is ```X``` with the extra column of 1s representing features $x_0$.\n",
    "\n",
    "No looping through samples necessary at all. No looping through features. None of that. Below I've fleshed out the function ```predH(theta,x,m)``` and you can literally set ```h``` with a one-liner according to the vectorized expression above. Enjoy.\n",
    "\n",
    "**[0.5 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6HSNPyg3EEZ"
   },
   "outputs": [],
   "source": [
    "def predH(theta,x,m):\n",
    "\n",
    "    h = np.zeros_like(x) #Set this variable correctly below - an array h to store the predicted value for each row x\n",
    "\n",
    "    #Fill in below - set h - this can and should be one line.\n",
    "    h = np.dot(x,theta)\n",
    "\n",
    "\n",
    "    #Stop filling in here\n",
    "\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPSfEXV-3EEZ"
   },
   "source": [
    "Note that if you run the function you've written with the values $(\\theta_0 = 1, \\theta_1 = 2)$ applied to ```XMOD```, and if you did it right, you should be getting (and seeing) a $(87 \\times 1)$ matrix of predicted values, as follows:\n",
    "\n",
    "$\n",
    "[[12.36], [ 8.46], [13.3 ], [18.68], \\ldots , [ 6.12], [ 4.34], [ 6.22], [14.94]]\n",
    "$\n",
    "\n",
    "If not, re-visit the function above. It needs to work correctly for the rest of this assignment to work correctly. Feel free to try out your function for your own purposes in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEZHRAGC3EEZ",
    "outputId": "7108e6b7-5834-4878-8432-c057d505b3ec"
   },
   "outputs": [],
   "source": [
    "#This code snippet is for YOU\n",
    "#It just calls the hypothesis function. Feel free to play around with this to see what your hypothesis function is doing.\n",
    "\n",
    "THETA = np.array([[1],[2]]) #Change this to whatever thetas you like\n",
    "\n",
    "PREDSVEC = predH(THETA,XMOD,M) #I wouldn't advise changing this\n",
    "print(np.array2string(PREDSVEC, separator=',').replace(\"\\n\",\"\")) #I wouldn't advise changing this - I'm using the array2string function to be able to display the array in a more visually accessible format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNrHdYS23EEZ"
   },
   "source": [
    "Select and run the cell below to see whether your code is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i2YUBnY13EEZ",
    "outputId": "b50fddb3-d16f-4c8e-b885-f2017005ec5b"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params7(predH,addX0)\n",
    "utils.testCell(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4phRQdv3EEZ"
   },
   "source": [
    "### 1.6.3 Computing the Cost - Vectorized Method\n",
    "\n",
    "The vectorized method of computing the cost using matrix multiplication is to express the cost function as a set of matrix operations given by:\n",
    "\n",
    "$$\\large\n",
    "J(\\theta) = \\frac{1}{2m}(X\\theta - y)^T(X\\theta - y)\n",
    "$$\n",
    "\n",
    "Noting that $X$ here is actually ```XMOD``` i.e. it is ```X``` with the extra column of 1s representing features $x_0$. The expression above could be broken down as:\n",
    "\n",
    "   - Compute the matrix expression $X\\theta - y$, where $X$ and $y$ are the same matrices (arrays) we created earlier (`X` and `Y`) and $\\theta$ is the matrix containing $(\\theta_0, \\theta_1, \\ldots, \\theta_n)$ that is being evaluated in the cost function. This, i.e. $X\\theta - y$, has the effect of computing $\\theta_1x^{(i)} + \\theta_0 - y^{(i)}$ for all $m$ samples **in one (shweet) go**. Let's assume we store the result of this matrix operation in a variable called ```costcalc```.\n",
    "   - Square all the results **and** add them up **in one (shhweet) go** by multiplying the transpose ```costcalc```$^T$ by the original ```costCalc``` (order matters here). Let's assume we store the result of this squaring and summing operation in a variable called ```costCalcSqr```.\n",
    "   - Finally, divide the result by $2m$. If need be, convert the type to a ```float```. Done.\n",
    "\n",
    "Note that you wouldn't even require a loop here. Once you're used to it, this procedure would literally be between 1 and about 4 or 5 lines of code (I did it in 1 line and so can you). This would not only be much faster (because packages like numpy are highly optimized to carry out a large number of matrix multiplications very quickly), but also shorter and more intuitive once you get a feel for it. More importantly, this method will work for a matrix $X$ with *any* number of features in it (and not just 1). This method is also called the 'Vectorized' solution since we're only working with vectors and matrices, and not looping over any individual samples. Feel free to use your linear algebra knowledge to confirm that this would be exactly the same as the iterative solution, or just take my word for it ;). You can also use a polite search engine to get more info if need be. Anyway, below, now go ahead and implement the **vectorized solution** I've described above, in the ```getCost(theta, x, y, m)``` function I've fleshed out below and make sure it returns the relevant cost as **a single float value** (and not an array containing only a single row and/or column, for example).\n",
    "\n",
    "**[2.5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3KnqQ7_3EEZ"
   },
   "outputs": [],
   "source": [
    "def getCost(theta, x, y, m):\n",
    "\n",
    "    cost = 0\n",
    "    #Fill in below - set cost correctly\n",
    "    difference = np.dot(x, theta) - y\n",
    "    cost = np.dot(difference.T, difference) / (2*m)\n",
    "    cost = cost.item()\n",
    "\n",
    "\n",
    "    #Stop filling in here\n",
    "\n",
    "    return np.squeeze(cost)#the cost as a single scalaar value and NOT an array (containing a single value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0Ofqtjy3EEZ"
   },
   "source": [
    "Again, if you run the function you've written with the values $(\\theta_0 = 1.87, \\theta_1 = 1.23)$ (which are the approximate best-fit solutions to the data) and if you did it right, you should be seeing the value (yes a single value and NOT an array) ```0.9289940...```. If not, re-visit the function above. The values of the iterative cost function (```getCostIter```) and vectorized cost function (```getCost```) should be the same given the same inputs, noting that ```getCostIter``` takes in ```X``` without the extra column of 1s and ```getCost``` takes in ```XMOD``` *with* the extra column of 1s; and also noting that ```getCost``` should work for $X$ with any number of features, but `getCostIter` probably only works for $X$ with one feature. Anyway, ```getCost``` absolutely needs to work correctly for the rest of this assignment to work correctly. A lot of things below rely on this function. Feel free to try out your function for your own purposes in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tVKaSF-3EEZ",
    "outputId": "855dc1de-a34b-4ffa-df3c-c405dfc413dc"
   },
   "outputs": [],
   "source": [
    "#This code snippet is for YOU\n",
    "#It just calls the cost function. Feel free to play around with this to see what your cost function is doing.\n",
    "THETA = np.array([[1.87],[1.23]]) #Change this to whatever thetas you like\n",
    "#Vectorized\n",
    "print(getCost(THETA,XMOD,Y,M)) #I wouldn't advise changing this\n",
    "\n",
    "#Iterative that you wrote before (hopefully)\n",
    "print(getCostIter(THETA,X,Y,M)) #I wouldn't advise changing this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmOK8OQU3EEa"
   },
   "source": [
    "Select and run the cell below to see whether your code is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bHpU2aj3EEa",
    "outputId": "b5790ee5-c978-42be-e092-41961131b56c"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params8(getCost,addX0)\n",
    "utils.testCell(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EJmYNgz3EEa"
   },
   "source": [
    "Just so you can see, the two cells below use the Jupyter magic command ```%timeit``` to time the two cost function alternatives. Run them and then compare the times. You should be able to see that the vectorized version is more than 10 times faster. The gains would be even greater as the number of example ```m``` increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7uM1SSE3EEa"
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    raise NotImplementedError(\"Skipped during marking.\")\n",
    "THETA = np.array([[0],[0]])\n",
    "%timeit getCost(THETA,XMOD,Y,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d1_vqmV3EEa"
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    raise NotImplementedError(\"Skipped during marking.\")\n",
    "THETA = np.array([[0],[0]])\n",
    "%timeit getCostIter(THETA,X,Y,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp0AUoyI3EEa"
   },
   "source": [
    "### 1.6.4 Visualizing the Cost Function\n",
    "\n",
    "Just for you to actually see it, we will display the cost function below. I won't get you to do this. I've provided you the code below that generates a contour plot of the cost function for this data. Feel free to look through the code to see how it works. I'm not going to generate a 3D surface though; rather it will generate a contour plot. Maybe you can see if you can generate a 3D surface (not for marks)??.\n",
    "\n",
    "Note that, in practice, if we have more than one feature in ```X```, we won't easily be able to draw the cost function, and we usually don't --- and in fact we don't even need to. Rather, as explained before, we simply use gradient descent which starts with some initial guess of the solution, and then continuously makes smart updates to that guess by taking descending steps on the cost function to get closer and closer to the best solution. There's no need to visualize that process. Therefore, the plot of the cost function below is strictly for (your) understanding purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 858
    },
    "id": "RN7bTloS3EEa",
    "outputId": "7242cd69-b0ad-495a-afc8-75703764f3b8"
   },
   "outputs": [],
   "source": [
    "#Run this snippet before moving on to the next parts\n",
    "#Don't change anything in this code snippet. But do look through it to see what it's doing.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from pylab import *\n",
    "import numpy.linalg as spla\n",
    "\n",
    "\n",
    "def plotCostFunction(x, y, bestthetas):\n",
    "\n",
    "#Generate a bunch of theta0 and theta1 values on a linear scale\n",
    "    theta0comb = np.linspace(-10, 10, 100);\n",
    "    theta1comb = np.linspace(-1, 4, 100);\n",
    "\n",
    "    #Create a 2D array to store the Jval corresponding to each unique theta0,theta1 pair\n",
    "    Jvals = np.zeros((len(theta0comb), len(theta1comb)))\n",
    "\n",
    "    # Compute the costs for each theta0,theta1 pair\n",
    "    for i in range(len(theta0comb)):\n",
    "        for j in range(len(theta1comb)):\n",
    "            thh = np.array([[theta0comb[i]],[theta1comb[j]]])\n",
    "            Jvals[j,i] = getCost(thh, x, y, len(y))\n",
    "\n",
    "\n",
    "    figure(figsize=(12,10))\n",
    "    xlabel(r'$\\theta_0$')\n",
    "    ylabel(r'$\\theta_1$')\n",
    "\n",
    "    #Draw the contour plot with cost levels that are spaced out logarithmically (for better viewing)\n",
    "    levels = np.logspace(-2,3,20)#np.array([0,2,6,10,14,20,28,36,46,56])#np.arange(-2, 30, 4)\n",
    "    contour(theta0comb, theta1comb, Jvals, levels, linewidths=3)\n",
    "\n",
    "    #Finally, draw on the theta values for which the cost function is minimum with an X\n",
    "    scatter(bestthetas[0,0], bestthetas[1,0], c='r', marker='x')\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    retthetas = spla.pinv(XMOD.T@XMOD)@XMOD.T@Y\n",
    "    plotCostFunction(XMOD, Y, retthetas)\n",
    "except Exception as e:\n",
    "    printmd(\"### Something went wrong. Maybe you haven't implemented the steps above correctly/yet?\")\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_kpJf7H3EEa"
   },
   "source": [
    "If you did everything correctly i.e. implementing all the previous vectorized functions correctly, you should see a cost function generated in the cell above that looks similar (actually the same) as the one in the image below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTqoPAuo3EEa"
   },
   "source": [
    "### 1.6.5 Implementing Gradient Descent - Vectorized Method\n",
    "\n",
    "Below I've fleshed out a function ```gradDesc(theta, x, y, m, k_iters, lr)``` for you. I've put in the main for loop that carries out `k_iters` iterations of updates to all $\\Theta$; you have to fill in code into the for loop to update all $\\Theta$ simultaneously using the vectorized expression (below). Note that, in this case, you shouldn't loop through any individual data samples as you might have done in the iterative version. Rather, just focus on using the vectorized expression below and the update to all $\\Theta$ will be done correctly. The update process can be expressed as the matrix operation:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\Theta = \\Theta - \\frac{\\alpha}{m}X^T(X\\Theta - y)\n",
    "$$\n",
    "\n",
    "where $\\Theta$ is a vector of ($\\theta_0$, $\\theta_1$, $\\ldots$, $\\theta_n$) values, $\\alpha$ is the learning rate, $m$ is the number of data samples, $X$ is a matrix containing your features but also includes a column of 1s as the first column representing feature $x_0$, $X^T$ is the transpose of $X$ and $y$ is the vector of output/target values. If you want to, feel free to confirm for yourself that the expression above is exactly the same as carrying out the update equations given in the previous section. Note that you can literally compute the above expression in one line of code.\n",
    "\n",
    "**[2.5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSLH2dUx3EEb"
   },
   "outputs": [],
   "source": [
    "def gradDesc(theta, x, y, m, k_iters, lr):\n",
    "\n",
    "    J_hist = []\n",
    "\n",
    "    #FILL IN Your code here: make a call to the Iterative Cost Function to get the cost:\n",
    "    costJ = getCost(theta, x, y, m) #Set this\n",
    "    #STOP FILLING IN here\n",
    "\n",
    "    J_hist.append(costJ)\n",
    "\n",
    "    for ite in range(k_iters):\n",
    "\n",
    "\n",
    "        #FILL IN Your code below - but there shouldn't be a loop in here\n",
    "        error = np.dot(x, theta) - y\n",
    "        gradient = np.dot(x.T, error) / m\n",
    "        theta = theta - (lr * gradient)\n",
    "\n",
    "        #STOP FILLING IN here\n",
    "\n",
    "        #FILL IN Your code below: make a call to the Iterative Cost Function to get the cost:\n",
    "        costJ = getCost(theta, x, y, m) #Set this\n",
    "\n",
    "        #STOP FILLING IN here\n",
    "\n",
    "        J_hist.append(costJ)\n",
    "\n",
    "    return J_hist, theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlAhJgRy3EEb"
   },
   "source": [
    "If you did it correctly, running ```gradDesc``` with initial $(\\theta_0=0,\\theta_1=0)$ with a ```lr=0.001``` over ```k_iters=100``` iterations, you should expect to end up with values of $\\theta_0 \\approx 0.28669954, \\theta_1 \\approx 1.35706138$, and this is exactly the same as running ```gradDescIter```, except that it is much faster and you can safely run it over a much larger number of iterations without clogging up your processor (although remember that ```gradDescIter``` takes in the original ```X``` that doesn't have the extra column of 1s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xaQ6F12T3EEb",
    "outputId": "4c93cb43-6954-4e71-dca1-0d81dbccf29b"
   },
   "outputs": [],
   "source": [
    "#This code snippet is for YOU\n",
    "#It just calls the gradient descent function. Feel free to play around with this to see what your function is doing.\n",
    "\n",
    "THETA = np.array([[0],[0]]) #Change this to whatever thetas you like\n",
    "LR = 0.001\n",
    "K_ITERS = 100 #Be careful about setting this too large with iterative gradient descent... it can take a LONG time to finish\n",
    "\n",
    "JHIST_1, THETA_VEC = gradDesc(THETA, XMOD, Y, M, K_ITERS, LR) #I wouldn't advise changing this\n",
    "\n",
    "print(THETA_VEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIKj9JAx3EEb"
   },
   "source": [
    "Now as before, select and run the cell below to see whether your code working correctly. The cell calls your function. Since this method is iterative and slow, it only runs it over a small number of iterations (100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDw2SANH3EEb",
    "outputId": "d6bec02c-b190-48f4-e622-860426707b19"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params9(gradDesc,addX0)\n",
    "utils.testCell(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lwxvJRi3EEb"
   },
   "source": [
    "Just so you can see, the two cells below use the Jupyter magic command ```%timeit``` to time the two gradient descent alternatives. Run them and then compare the times. You should be able to see that the vectorized version is more than 10 times faster (it was actually about 17 times faster on my own machine). The gains would be even greater as the number of examples ```m``` increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vk8xS1TV3EEb"
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    raise NotImplementedError(\"Skipped during marking.\")\n",
    "THETA = np.array([[0],[0]])\n",
    "%timeit gradDesc(THETA, XMOD, Y, M, 100, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5FOaCfk3EEb"
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    raise NotImplementedError(\"Skipped during marking.\")\n",
    "THETA = np.array([[0],[0]])\n",
    "%timeit gradDescIter(THETA, X, Y, M, 100, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxXuXumM3EEb"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x8X3dCi3EEb"
   },
   "source": [
    "## 1.7 Plotting the Cost $J$ versus Iteration Number\n",
    "\n",
    "Assuming you implemented gradient descent correctly and it works, it should be able to return a list which contains the cost at each iteration of gradient descent as it runs. Plotting and observing this plot is key to determining if gradient descent is converging and whether the learning rate is: too small, too large, or just right. In practice, you will usually draw this plot as gradient descent runs in order to keep an eye on it and see \"how things are going\". We won't do that here (but feel free to see if you can create your own modified version of the vectorized gradient descent function that draws a plot of the cost at each iteration as gradient descent is running - NOT for marks).\n",
    "\n",
    "In the code segment below, I've first made a call to your (vectorized) gradient descent function. Then, you need to  write the code to plot a graph of the cost $J$ versus the iteration number. All the data you will need is stored inside the variable ```RETJHIST```. Preferably use ```matplotlib```.\n",
    "\n",
    "**[2 marks]**\n",
    "\n",
    "To check that you're correct, given a learning rate of 0.005 and only 50 iterations in the function call below, your eventual graph should end up looking something like the figure below. But once you've got it right, feel free to continuously change the learning rate to see the effect; aim to keep increasing the learning rate until just before you see the cost $J$ skyrocket (instead of reduce). E.g. see what happens when you set the learning rate to 0.08444.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "8YEvR7Fv3EEc",
    "outputId": "077f5e4b-75c2-4dc0-adc9-61b225e652d6"
   },
   "outputs": [],
   "source": [
    "#Don't remove or modify the two lines below\n",
    "THETA = np.array([[0],[0]]) #Creates a 2x1 column vector of zeros\n",
    "RETJHIST, RETTHETA = gradDesc(THETA, XMOD, Y, M, 50, 0.005)  #Don't remove this, but feel free to change the learning rate\n",
    "\n",
    "\n",
    "#Fill in below Import any libraries you may need for plotting inside the function here\n",
    "import matplotlib.pyplot as plt\n",
    "#Stop filling in here\n",
    "\n",
    "\n",
    "def plotCostJ(jhist):\n",
    "    #Fill in below\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    ax.plot(range(len(jhist)), jhist, marker = 'o', color='red', linewidth=2)\n",
    "    ax.set_title('Cost vs. Iteration Number')\n",
    "    ax.set_xlabel('Iteration Number')\n",
    "    ax.set_ylabel('Cost J(Theta)')\n",
    "    ax.grid(True)\n",
    "    #Stop filling in here\n",
    "    return\n",
    "\n",
    "plotCostJ(RETJHIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJyUUWrN3EEc"
   },
   "source": [
    "## 1.8 Making Predictions Using the $\\theta$ (Model) Obtained\n",
    "\n",
    "Now that we've hopefully implemented gradient descent and it has given us a good set of $\\Theta_\\text{best}$ to use to make predictions, we can go ahead and make predictions on some new set of data $X_\\text{test}$. To do this, and noting that in this Section we only had one feature in $X$ and so we didn't need to do any feature scaling before carrying out gradient descent, we can go ahead and make predictions on new samples $X_\\text{test}$ by simply doing the following:\n",
    "\n",
    "1. Add feature $x_0$ to $X_\\text{test}$.\n",
    "2. Call the function ```predH``` written earlier with the $\\Theta_\\text{best}$ on the scaled $X_\\text{test}$ with $x_0$.\n",
    "\n",
    "Below, I've fleshed out a function ```makeAPredictionNoScaling(xtest, n, m, theta)``` that takes in the ```xtest``` that we're trying to predict on, coupled with the ```theta``` parameters (probably obtained via gradient descent previously), and then the function should proceed to do the following:\n",
    "\n",
    "1. Uses the function ```addX0``` to add feature $x_0$ to ```xtest``` to get ```xtestmod```\n",
    "2. Passes ```xtestmod``` to function ```predH``` to get the predicted values in ```ytestpred```.\n",
    "3. Returns ```ytestpred```.\n",
    "\n",
    "It's literally a bunch of function calls. If you got this far, this should be easy.\n",
    "\n",
    "**[0.5 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXYVRLzG3EEc"
   },
   "outputs": [],
   "source": [
    "def makeAPredictionNoScaling(xtest, n, m, theta):\n",
    "\n",
    "    #Step 1: call addX0 to add feature x0 to xtestsc\n",
    "    xtestmod = addX0(xtest, m)\n",
    "\n",
    "    #Step 2: call predH to get predicted values for each example in xtestmod\n",
    "    ytestpred = predH(theta, xtestmod, m)\n",
    "\n",
    "    #Step 3: Return\n",
    "    return ytestpred #DON'T CHANGE THIS LINE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTKAOIcQ3EEc"
   },
   "source": [
    "To test out your function, you can call it with `X` and the $\\Theta_\\text{best}$ given below, and you should see that the predicted prices are extremely close to those in `Y`.\n",
    "\n",
    "$$\n",
    "\\Theta_\\text{best} = \\left[\\begin{array}[l]  _1.8667\\\\\n",
    " 1.2311\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "Also, given:\n",
    "\n",
    "$$\n",
    "X_\\text{test} = \\left[\\begin{array}[lll] _4.33 \\\\\n",
    "2.81\\\\\n",
    "-2.57\\\\ \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "the predicted $y_\\text{test\\_pred}$ values should be approximately:\n",
    "\n",
    "$$\n",
    "y_\\text{test\\_pred} \\approx \\left[\\begin{array}{l}  7.197\\\\\n",
    " 5.326\\\\\n",
    " -1.297 \\end{array} \\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWIswygm3EEc",
    "outputId": "55b454b5-a945-4ba4-ab28-751958a8b14d"
   },
   "outputs": [],
   "source": [
    "#This cell is for YOU to try your code.\n",
    "\n",
    "THETATRY = np.array([[1.8667], [1.2311]])\n",
    "XTRY = np.array([[4.33], [2.8], [-2.57]])\n",
    "\n",
    "makeAPredictionNoScaling(XTRY,1,3,THETATRY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_f3FY8jv3EEc"
   },
   "source": [
    "Select and run the cell below to see whether your code working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Blja2zeW3EEc",
    "outputId": "c2e31a4a-4616-4e69-b133-77a81432c455"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params10(makeAPredictionNoScaling)\n",
    "utils.testCell(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8rtYeD93EEc"
   },
   "source": [
    "# 2 - Multivariate Linear Regression\n",
    "\n",
    "In this part, we're now going to consider a problem in which we're using more than one feature to predict the output. Consider the problem of trying to predict the price of a vehicle (in Rand) based on the vehicle's age (in years), mileage (in kms) and popularity index (which is some arbitrary index ranging from 1-10). The popularity index is a real-valued number between 1-10 which is measured by the national vehicle commission (made-up :D) by considering the overall demand for, and hype around, a particular vehicle type on the market at a given time. We've supposedly gone out and collected this data for 43 vehicles (found in file ```data2.csv```). We want to create a predictive model that can predict the price of a vehicle (in Rand) given the vehicle's age (in years), mileage (in kms) and popularity index at a given time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnmpIXNB3EEc"
   },
   "source": [
    "## 2.1 Reading in the Data and Setting $n$, $m$, $y$ and $X$\n",
    "\n",
    "I've put in the code below to read in the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0O_RvBbS3EEc"
   },
   "outputs": [],
   "source": [
    "#Run this snippet before moving on to the next parts\n",
    "#Don't change anything in this code snippet. But do look through it to see what it's doing.\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True) #This just prevents numpy from jumping into scientific display mode for very large/small numbers\n",
    "\n",
    "DATA2 = np.genfromtxt('data2.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gnXvQhe3EEd"
   },
   "source": [
    "The data file consists of three columns. The data is displayed below to demonstrate that it is read in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GINc8Zw13EEd",
    "outputId": "d44f384a-7d9d-4c68-c489-a5ca5c1872fc"
   },
   "outputs": [],
   "source": [
    "DATA2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8FYgB6Y3EEd"
   },
   "source": [
    "We'll also proceed to use your `obtainNandM` and `obtainYandX` functions to get $n$, $m$, $y$ and $X$ for this part as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jw1DYtm43EEd",
    "outputId": "96521fe3-4cf3-4830-cfff-bb9ebac6c836"
   },
   "outputs": [],
   "source": [
    "NN,MM = obtainNandM(DATA2)\n",
    "YY,XX = obtainYandX(DATA2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tE8TwWat3EEd"
   },
   "source": [
    "Just run the cell below which just uses your previously written functions ```obtainNandM``` and ```obtainYandX``` to get $n$, $m$, $y$ and $X$ for this new data set. The cell below will tell you if your ```obtainNandM``` and ```obtainYandX``` operate correctly on ```DATA2```. If so, continue with the assignment. If not, you will meet with problems lower down on this assignment, and you should rather focus on fixing the problem observed. No marks for this; this is just a check for YOU to make sure everything is in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IgQo5XJw3EEd",
    "outputId": "66ca2bf5-ba89-432b-8bcd-e53ad5b96869"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params11(obtainNandM,obtainYandX,DATA2)\n",
    "utils.testCell(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiLqpnfw3EEd"
   },
   "source": [
    "## 2.2 Feature Scaling\n",
    "\n",
    "When working with multiple features, feature scaling becomes very important. We didn't scale our data in Section 1 above because our data only had one feature. The data we're working with here, though, has 3 features. Feature scaling is especially important when the ranges of different features differ by orders of magnitude, and hopefully you can see that this is, in fact, the case with our data in this section. As discussed in class, feature scaling can be achieved by means of the following expression applied to each and every entry in $x_j^{(i)}$ in $X$:\n",
    "\n",
    "$$\n",
    "x_j^{(i)} = \\frac{x_j^{(i)}-\\mu_j}{s_j}    \\text{ for all $m$ data samples}\n",
    "$$\n",
    "\n",
    "where $x_j^{(i)}$ is the $j$th feature of the $i$th data sample, $\\mu_j$ is the average/mean value of feature $x_j$ across all the data samples, and $s_j$ is the standard deviation of values of $x_j$ across all the data samples (although the range can also be used - don't use it here).\n",
    "\n",
    "Below, I've fleshed out a function ```obtainScaledX(x,n)```. Complete the function to scale the features of the input array ```x```. Note that the input array ```x``` that this function expects **excludes** the extra column of 1s for feature $x_0$ i.e. in your function, assume that ```x``` doesn't have feature $x_0$ which is a column of 1s, and which shouldn't be scaled in any case. The function provides two variables ```means``` and ```stdevs``` which are both initialized to $(1\\times n)$ row vectors, because they will contain, respectively, the mean and standard deviation of each column in ```x```. Put the scaled values of ```x``` inside the ```xsc``` array provided in the function. ```xsc``` is returned by the function. You could follow the following steps, in which case there will be one loop below:\n",
    "\n",
    "For each feature (i.e. column) in ```x```:\n",
    "    \n",
    "1. Compute the mean (```mean```) of the column and store it in the corresponding column entry of variable ```means``` provided.\n",
    "    \n",
    "2. Now subtract the mean of this column from the entire column and store the result in the corresponding column of ```xsc```.\n",
    "\n",
    "3. Compute the standard deviation of the column and store it in the corresponding column entry of variable ```stdevs``` provided.\n",
    "\n",
    "4. Divide the entire column by this standard deviation and store the result in the corresponding column in ```xsc```.\n",
    "\n",
    "If you use this method, note that, while you will be looping through the features/columns, you shouldn't be looping through the individual samples. Rather, use matrix arithmetic to apply feature scaling to all the feature values at the same time e.g. subtracting a numpy array $A$ by a fixed value $b$ results in subtracting every individual item in $A$ by $b$ in one go; this is called \"broadcasting\". Feel free to look up how broadcasting works in ```numpy```. The same is true of division, multiplication and addition. So consider applying ```mean```, ```std``` and other operations directly to entire columns of ```X``` e.g. ```X[:,0:1]``` is the entire first column of ```X```.\n",
    "\n",
    "There is an alterative way to carry out scaling in one line of code, which will involve using `numpy` indexing and functions.\n",
    "\n",
    "Finally, the function returns ```xsc```, but also ```means``` and ```stdevs```. These need to be returned so they can be applied to future new test samples that need to first be scaled before they can be passed into the final hypothesis of this model. So given some new sample (i.e. vehicle) with a given age, mileage and popularity index, we would need to apply the feature scaling procedure to the sample before passing it through our hypothesis to make a prediction on its price. This is because the model will have been trained on scaled data. We need to be consistent.\n",
    "\n",
    "Hint: ```numpy```'s ```mean``` and ```std``` functions may be useful here.\n",
    "\n",
    "\n",
    "\n",
    "**[2.5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJzGXoxd3EEd"
   },
   "outputs": [],
   "source": [
    "def obtainScaledX(x,n):\n",
    "\n",
    "    xsc = np.zeros_like(x)\n",
    "    means = np.zeros((1,n))\n",
    "    stdevs = np.zeros((1,n))\n",
    "\n",
    "    #Don't change anything above this point.\n",
    "    #Start filling in below with your feature scaling code. Follow instructions above\n",
    "    for i in range(n):\n",
    "        col = x[:,i]\n",
    "\n",
    "        mean = np.mean(col)\n",
    "        means[0, i] = mean\n",
    "\n",
    "        stdev = np.std(col)\n",
    "        stdevs[0, i] = stdev\n",
    "\n",
    "        xsc[:, i] = (col - mean) / stdev\n",
    "    #Stop filling in here. Don't change anything below\n",
    "\n",
    "    return stdevs,means,xsc\n",
    "\n",
    "STD,MN,XSC = obtainScaledX(XX,NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihYNpdRP3EEd",
    "outputId": "d9f3f2fb-f7d0-40aa-844c-8843004940a4"
   },
   "outputs": [],
   "source": [
    "#This code snippet is for YOU\n",
    "#It just calls the feature scaling function. Feel free to play around here to see what your function is doing.\n",
    "STDEVS,MEANS,XSCALED = obtainScaledX(XX,NN) #I wouldn't advise changing this, but you can if you want to\n",
    "print(STDEVS)\n",
    "print(MEANS)\n",
    "print(XSCALED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtbIV7yd3EEd"
   },
   "source": [
    "Select and run the cell below to see whether your code working correctly. The cell below indicates whether or not your feature scaling function is apparently working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_sCsiawt3EEd",
    "outputId": "79b86ff1-65de-45c9-f01e-0033cd15f01f"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params12(obtainScaledX)\n",
    "utils.testCell(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y22mLfGu3EEd"
   },
   "source": [
    "## 2.3 Adding Feature $\\large x_0$\n",
    "\n",
    "Here, we'll just call your ```addX0``` function to add the feature $\\large x_0$ to the scaled $X$ so we can proceed to carry out gradient descent on it. Specifically, ```addX0``` will be run on ```XSCALED``` to obtain ```XXMOD``` which is the scaled version of $X$ with feature $\\large x_0$.\n",
    "\n",
    "Just run the cell below which will tell you if it operates correctly. If so, continue with the assignment. If not, you will meet with problems lower down on this assignment, and you should rather focus on fixing the problem observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prubSZ7w3EEe",
    "outputId": "4f6f18e6-3706-4bf1-ac7d-25f5e7c48e04"
   },
   "outputs": [],
   "source": [
    "XXMOD = addX0(XSCALED,MM)\n",
    "print(XXMOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFbECBwB3EEe"
   },
   "source": [
    "Select and run the cell below to see whether your code is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGzQ6LAR3EEe",
    "outputId": "bfedf94c-1436-4a00-c9e2-b075506a4dc1"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params13(addX0)\n",
    "utils.testCell(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcPCOczP3EEe"
   },
   "source": [
    "## 2.4 Gradient Descent for Multiple Features\n",
    "\n",
    "Now that the features are scaled, we can perform gradient descent for multiple features. You might be very pleased to hear that if you implemented the vectorized version of gradient descent for one variable earlier on, you can simply use that function with more features, and it *should* work. No coding required here. We'll just call the function and it *should* just work. If you run ```gradDesc``` for 10000 iterations on ```XXMOD``` and ```YY``` with initial $\\Theta = (\\theta_0=0,\\theta_1=0,\\ldots,\\theta_3=0)$ and $\\alpha=0.01$, you should end up with the approximate best-fit parameters for this data set of:\n",
    "\n",
    "$$\n",
    "\\Theta_\\text{best} = \\left[  \\begin{array}[l]  _\\theta_0\\approx478510.3 \\\\ \\theta_1\\approx-6.714 \\\\ \\theta_2\\approx-249916.5 \\\\ \\theta_3\\approx0.8857 \\end{array} \\right ]\n",
    "$$\n",
    "\n",
    "**[2 marks if it works]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzPFfUrw3EEe",
    "outputId": "4c2ab48d-c90e-4e39-be99-b1b655234085"
   },
   "outputs": [],
   "source": [
    "#This cell is for YOU to try out\n",
    "\n",
    "#This code snippet is for YOU\n",
    "#It just calls the gradient descent function. Feel free to play around with this to see what your function is doing.\n",
    "\n",
    "THETA = np.zeros((NN+1,1)) #Create a column vector of zeros of size ((n+1)x1) i.e. set theta_0, theta_1... theta_n initially to 0\n",
    "LR = 0.01\n",
    "K_ITERS = 10000 #You should be able to set this nice and high if you like, but if you overdo it, your notebook may freeze.\n",
    "\n",
    "JHIST_P2, THETA_VEC_P2 = gradDesc(THETA, XXMOD, YY, MM, K_ITERS, LR) #I wouldn't advise changing this\n",
    "\n",
    "print(THETA_VEC_P2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWTkBE2g3EEe"
   },
   "source": [
    "Select and run the cell below to see whether your code is apparently working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7H53UFBg3EEe",
    "outputId": "c81e943b-caa8-4e7c-dd6e-e3d36343c90f"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params14(gradDesc)\n",
    "utils.testCell(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtMc5MGU3EEe"
   },
   "source": [
    "## 2.5 Plotting the Cost $J$ versus Iteration Number and Comparing Learning Rates\n",
    "\n",
    "As with Section 1, assuming you implemented gradient descent correctly and it works, it should be able to return a list which contains the cost at each iteration of gradient descent as it runs. Plotting and observing this plot is key to determining if gradient descent is converging and whether the learning rate is: too small, too large, or just right. In practice, you will usually draw this plot as gradient descent runs in order to keep an eye on it and see \"how things are going\". We won't do that here (but feel free to see if you can create your own modified version of the vectorized gradient descent function that draws a plot of the cost at each iteration as gradient descent is running - NOT for marks).\n",
    "\n",
    "Previously (in Section 1), we plotted a graph of $J$ versus iteration number for a single learning rate. While that plot *can* be used to compare learning rates, this would require us to keep changing the learning rate, re-running gradient descent and re-plotting the graph in order to see how convergence is changing. This is not ideal. Below, I've graciously put in code that plots the cost versus the iteration number of gradient descent **for a series of different learning rates**. The function can plot the cost plot for up to 7 different learning rates on a single graph, and the function runs gradient descent by itself, so there is no need to do that manually either.\n",
    "\n",
    "Look at the graphs, and make sure to make changes to the learning rates in the ```alphatotry``` list and compare different learning rates and determine the best learning rate to use. As explained in class, when the learning rate is small, the cost reduces very slowly. As you increase the learning rate, convergence become faster and faster i.e. the cost plot drops more and more dramatically to a stable minimum; from a specific learning rate onwards, as the learning rate is set too high, the plot may still converge but convergence starts to become slower and slower as gradient descent starts to overshoot the minimum but still reduces the cost on every iteration; then at some point (when the learning rate is just too high), the cost plot explodes / diverges. Keep changing values in ```alphatotry``` (maximum seven entries) to try and observe this trend. Start small and gradually increase the learning rate; initially it will start to converge faster and faster; then convergence will start to become less and less steep; at some point you should see that it explodes. The learning rate that causes the explosion is obviously too high. Then work your way down systematically to the point where it doesn't explode (converges) and also does so the fastest. This is for marks; you will have to put the best learning rate you determined into a function lower down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "id": "CuVAtVFk3EEe",
    "outputId": "2ba0f3a8-fb2c-42ae-fe3e-f3602a0a9978"
   },
   "outputs": [],
   "source": [
    "#Don't change anything except variable ``alphatotry`` in this code snippet (unless you really want to). Note that\n",
    "#you can have up to seven learning rates in the ``alphatotry`` list, which the plot will automatically draw\n",
    "#and you can then compare. You can, however, have fewer than seven items in the list too.\n",
    "#But do look through it to see what it's doing.\n",
    "\n",
    "alphatotry = [0.001, 0.007, 0.01, 0.02, 1.15 , 1.1 , 1.2]\n",
    "\n",
    "#Don't edit anything below this point (unless you insist on doing so)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from pylab import *\n",
    "\n",
    "def plotLearningRates(x,y,alphas):\n",
    "\n",
    "    plotcol = ['r','g','b','y','c','m','k']\n",
    "\n",
    "    retJhistforalphas = []\n",
    "    for alphait in alphas:\n",
    "        retJ_hist,_ = gradDesc(np.zeros((x.shape[1],1)), x, y, len(x), 50, alphait)\n",
    "        retJhistforalphas.append(retJ_hist)\n",
    "\n",
    "    xplot = np.linspace(1,len(retJhistforalphas[0]),int(len(retJhistforalphas[0])))\n",
    "\n",
    "    figure(figsize=(12,10))\n",
    "\n",
    "    for it, retJhistit in enumerate(retJhistforalphas):\n",
    "        plot(xplot, np.array(retJhistit), c=plotcol[it], marker='x',label=str(alphas[it]))\n",
    "\n",
    "    xlabel('Iteration No.')\n",
    "    ylabel('Cost $ J$')\n",
    "    legend()\n",
    "    show()\n",
    "\n",
    "try:\n",
    "    plotLearningRates(XXMOD, YY, alphatotry)\n",
    "except Exception as e:\n",
    "    printmd(\"### Something went wrong.\")\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUTL42rE3EEe"
   },
   "source": [
    "Ok now in the function below, you will need to set the value of alpha to the best learning rate you determined. This is for marks. The closer your answer is to the correct learning rate, the more marks you get. If your learning rate causes a divergence of gradient descent, you get nothing. No immediate feedback cell for this one either.\n",
    "\n",
    "**[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VKshnuT3EEe"
   },
   "outputs": [],
   "source": [
    "def setLearningRatePart2():\n",
    "\n",
    "    alpha = 1.1 #Set this correctly; set this to the best learning rate\n",
    "\n",
    "\n",
    "    return alpha #Don't change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4APPs3zc3EEe"
   },
   "source": [
    "## 2.6 Making Predictions Using the $\\theta$ (Model) Obtained\n",
    "\n",
    "Now that we've hopefully implemented gradient descent and it has given us a good set of $\\Theta_\\text{best}$ to use to make predictions, we can go ahead and make predictions on some new set of data $X_\\text{test}$. To do this, and noting that in this Section we had to carry out feature scaling in order to make gradient descent obtain a solution faster, we will need to do the following:\n",
    "\n",
    "1. Scale $X_\\text{test}$ using the **same** scaling parameters obtained from the training set.\n",
    "2. Add feature $x_0$ to $X_\\text{test}$.\n",
    "3. Call the function ```predH``` written earlier with the $\\Theta_\\text{best}$ on the scaled $X_\\text{test}$ with $x_0$.\n",
    "\n",
    "Therefore, below we'll do this in two parts: we'll first write a function to scale a given $X_\\text{test}$ using existing scaling parameters; then we'll write a function that carries out all three steps listed above to make predictions.\n",
    "\n",
    "### 2.6.1 Feature Scaling of an $X_\\text{test}$ Using Existing Scaling Parameters\n",
    "\n",
    "You might be pleased to know that doing this is literally as simple as applying the expression below, and in this case we already have $\\mu_j$ and $s_j$, which we would have obtained from our previously written ```obtainScaledX``` function. It will therefore be extremely similar to the previous scaling function we wrote.\n",
    "\n",
    "$$\n",
    "x_j^{(i)} = \\frac{x_j^{(i)}-\\mu_j}{s_j}    \\text{ for all $m$ data samples}\n",
    "$$\n",
    "\n",
    "where $x_j^{(i)}$ is the $j$th feature of the $i$th data sample, $\\mu_j$ is the average/mean value of feature $x_j$ across all the data samples, and $s_j$ is the standard deviation of values of $x_j$ across all the data samples (although the range can also be used - don't use it here).\n",
    "\n",
    "Below, I've fleshed out a function ```obtainScaledXTest(x,n,stdevs,means)```. Complete the function to scale the features of the input array ```x```. Note that the input array ```x``` that this function expects **excludes** the extra column of 1s for feature $x_0$ i.e. in your function, assume that ```x``` doesn't have feature $x_0$ which is a column of 1s, and which shouldn't be scaled in any case. The function additionally **takes in** ```means``` and ```stdevs``` which are both $(1\\times n)$ row vectors obtained previously, and they will contain, respectively, the mean and standard deviation of each column in ```x```. Put the scaled values of ```x``` inside the ```xsc``` array provided in the function. ```xsc``` is returned by the function. You could follow the following steps (and note that there is one loop below):\n",
    "\n",
    "For each feature (i.e. column) in ```x```:\n",
    " 1. Subtract the mean of this column from the entire column and store the result in the corresponding column of ```xsc```.\n",
    " 2. Divide the entire column by this standard deviation and store the result in the corresponding column in ```xsc```.\n",
    "\n",
    "You could also do it in one line using `numpy`. Finally, the function returns ```xsc``` only.\n",
    "\n",
    "Hint: This will look almost identical to what you had in the ```obtainScaledX``` function before.\n",
    "\n",
    "While you might be looping through the features/columns, you shouldn't be looping through the individual samples. Rather, use matrix arithmetic to apply feature scaling to all the feature values at the same time e.g. subtracting a numpy array $A$ by a fixed value $b$ results in subtracting every individual item in $A$ by $b$ in one go; this is called \"broadcasting\". Feel free to look up how broadcasting works in ```numpy```. The same is true of division, multiplication and addition. So consider applying ```mean```, ```std``` and other operations directly to entire columns of ```X``` e.g. ```X[:,0:1]``` is the entire first column of ```X```.\n",
    "\n",
    "**[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0CN18lz3EEf"
   },
   "outputs": [],
   "source": [
    "def obtainScaledXTest(x,n,stdevs,means):\n",
    "\n",
    "    xsc = np.zeros_like(x)\n",
    "\n",
    "    #Don't change anything above this point.\n",
    "    #Start filling in below with your feature scaling code. Follow instructions above\n",
    "    for i in range(n):\n",
    "        col = x[:,i]\n",
    "        mean = means[0, i]\n",
    "        stdev = stdevs[0, i]\n",
    "\n",
    "        xsc[:, i] = (col - means[0, i]) / stdevs[0, i]\n",
    "\n",
    "    #Stop filling in here. Don't change anything below\n",
    "\n",
    "    return xsc\n",
    "\n",
    "XTESTSC = obtainScaledXTest(XX,NN,STD,MN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5P-d5B63EEf"
   },
   "source": [
    "To test out your function, given:\n",
    "\n",
    "$$\n",
    "X_\\text{test} = \\left[\\begin{array}[ll] _2 & 3\\\\\n",
    "4 & 5\\\\\n",
    "6 & 7\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "\\mu = \\left[\\begin{array}[l] _2.6 & 3.4\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\text{StdDev} = \\left[\\begin{array}[l] _0.4 & 0.5\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "should give you approximately:\n",
    "\n",
    "$$\n",
    "X_\\text{test\\_scaled} \\approx \\left[\\begin{array}[r] _-1.5 & -0.8\\\\\n",
    "3.5 & 3.2\\\\\n",
    "8.5 & 7.2\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Hint: If you run this example, make sure that your setup your $\\mu$ and StdDev arrays such that the ```shape``` of these arrays is ```(1,2)```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCgYkeGS3EEf",
    "outputId": "e3ec20ad-22f2-4806-b6d9-ff2fc5a1896f"
   },
   "outputs": [],
   "source": [
    "#This code snippet is for YOU\n",
    "#It just calls the feature scaling function. Feel free to play around here to see what your function is doing.\n",
    "XTESTSCALED = obtainScaledXTest(XX,NN,STD,MN) #I wouldn't advise changing this\n",
    "print(XTESTSCALED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-D53Zjx3EEf"
   },
   "source": [
    "Select and run the cell below to see whether your code working correctly. The cell below indicates whether or not your feature scaling function is apparently working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OOp0RiJk3EEf",
    "outputId": "3291a3e8-6215-4a03-a311-40322c309241"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params15(obtainScaledXTest)\n",
    "utils.testCell(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QltgC98E3EEf"
   },
   "source": [
    "### 2.6.2 Making the Prediction (With Scaling)\n",
    "\n",
    "Below, I've fleshed out a function ```makeAPredictionWithScaling(xtest, n, m, theta, stdevs, means)``` that takes in scaling parameters ```stdevs``` and ```means``` along with the ```xtest``` that we're trying to predict on, coupled with the ```theta``` parameters (probably obtained via gradient descent previously), and then the function should proceed to:\n",
    "\n",
    "1. Calls the ```obtainScaledXTest``` function on ```xtest``` with ```stdevs``` and ```means``` to get the scaled ```xtestsc```.\n",
    "2. Uses the function ```addX0``` to add feature $x_0$ to ```xtestsc``` to get ```xtestmod```\n",
    "3. Passes ```xtestmod``` to function ```predH``` to get the predicted values in ```ytestpred```.\n",
    "4. Returns ```ytestpred```.\n",
    "\n",
    "It's literally a bunch of function calls. If you got this far, this should be easy.\n",
    "\n",
    "**[1.5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtBQoCSg3EEf"
   },
   "outputs": [],
   "source": [
    "def makeAPredictionWithScaling(xtest, n, m, theta, stdevs, means):\n",
    "\n",
    "    #Step 1: call obtainScaledXTest to set xtestsc correctly\n",
    "    xtestsc = obtainScaledXTest(xtest, n, stdevs, means)\n",
    "\n",
    "    #Step 2: call addX0 to add feature x0 to xtestsc\n",
    "    xtestmod = addX0(xtestsc, m)\n",
    "\n",
    "    #Step 3: call predH to get predicted values for each example in xtestmod\n",
    "    ytestpred = predH(theta, xtestmod, m)\n",
    "\n",
    "    #Step 4: Return\n",
    "    return ytestpred #DON'T CHANGE THIS LINE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o99iR2i3EEf"
   },
   "source": [
    "To test out your function, you can call it with `XX` and the $\\Theta_\\text{best}$ given below, and you should see that the predicted prices are extremely close to those in `YY`.\n",
    "\n",
    "$$\n",
    "\\Theta_\\text{best} = \\left[\\begin{array}[c]  _478510\\\\\n",
    " -6.714\\\\\n",
    " -249916\\\\\n",
    " 0.8857 \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "Also, given:\n",
    "\n",
    "$$\n",
    "X_\\text{test} = \\left[\\begin{array}[lll] _5.0 & 28000.0 & 7.89\\\\\n",
    "2.0 & 98000.0 & 4.89\\\\\n",
    "30.0 & 280000.0 & 9.89\\\\ \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "the predicted $y_\\text{test\\_pred}$ values should be approximately:\n",
    "\n",
    "$$\n",
    "y_\\text{test\\_pred} \\approx \\left[\\begin{array}[l]  _839811\\\\\n",
    " 659304\\\\\n",
    " 189968 \\end{array} \\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FI-iD3qn3EEf",
    "outputId": "8f62d99a-0226-4932-d755-f5eabc21be14"
   },
   "outputs": [],
   "source": [
    "#This cell is for YOU to try your code.\n",
    "\n",
    "THETA = np.zeros((NN+1,1))\n",
    "JHIST, THETATRY = gradDesc(THETA, XXMOD, YY, MM, 100, 0.1) #Run gradient descent to get the best THETAS again\n",
    "\n",
    "makeAPredictionWithScaling(XX,NN,MM,THETATRY,STD,MN) #Use the best THETAS to make predictions\n",
    "\n",
    "THETATRY = np.array([[478510], [-6.714], [-249916], [0.8857]])\n",
    "XTRY = np.array([[5.0, 28000.0, 7.89], [2.0, 98000.0, 4.89], [30.0, 280000.0, 9.89]])\n",
    "\n",
    "makeAPredictionWithScaling(XTRY,3,3,THETATRY,STD,MN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R5TL6Yx3EEf"
   },
   "source": [
    "Select and run the cell below to see whether your code working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2MERo8H3EEf",
    "outputId": "72d6feed-18b5-4541-9197-c0967935691e"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "params = utils.params16(makeAPredictionWithScaling)\n",
    "utils.testCell(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdMV5_Gn3EEf"
   },
   "source": [
    "***\n",
    "## 3. Using sklearn for Linear Regression\n",
    "\n",
    "It is effortless. The small snippet of code below does **exactly** what we've been doing up to now. Note that `sklearn`'s `model.fit` expects $X$ that isn't scaled and doesn't contain feature $x_0$. You can feed in the raw features and it do everything for you...\n",
    "\n",
    "Anyway, analyze the code below to determine how one can get the $\\Theta$ parameters using sklearn for yourself.\n",
    "\n",
    "Hint: Remember that $\\theta_0$ is actually the y-intercept of the hypothesis.\n",
    "\n",
    "Note that since we didn't apply any scaling, the $\\Theta$ values we get here will be different to those we obtained above when we applied gradient descent to the scaled $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "_NHdJJM-3EEg",
    "outputId": "e972e3eb-a90a-484c-9924-40cfd37f10e4"
   },
   "outputs": [],
   "source": [
    "#Run this snippet just to see what it does. You may also play around with it if you want\n",
    "if DEBUG:\n",
    "    raise NotImplementedError(\"Skipped during marking\")\n",
    "\n",
    "import sklearn.linear_model as lm\n",
    "model = lm.LinearRegression()\n",
    "try:\n",
    "    model.fit(XX,YY)\n",
    "    printmd(\"The y-intercept i.e. $\\\\theta_0$ is \" + str(model.intercept_))\n",
    "    printmd(\"The coefficients i.e. \" + \", \".join([\"$\\\\theta_%i$\" % i for i in range(1,NN+1)]) + \" are \" + np.array2string(model.coef_,separator=\", \"))\n",
    "except Exception as e:\n",
    "    printmd(\"### Something went wrong. Check your function.\")\n",
    "    print(str(e))\n",
    "\n",
    "printmd(\"***\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VbqmVfN3EEg"
   },
   "source": [
    "And making a prediction is just as effortless. We'll make predictions on ``XX``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5QXHRjWW3EEg",
    "outputId": "5a25de1d-7ec7-489f-a582-a7f995f72881"
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    raise NotImplementedError(\"Skipped during marking\")\n",
    "\n",
    "print(model.predict(XX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmjLiKNq3EEg"
   },
   "source": [
    "\n",
    "**DONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OrrY_6R3EEg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
